{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbd73b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "base_dir = os.getenv('BASEDIR')\n",
    "import re\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f995e90",
   "metadata": {},
   "source": [
    "### Qanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2739fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 103074/103074 [00:55<00:00, 1869.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name='qanda'\n",
    "data = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','qanda','qanda_episodes.csv'), dtype=str)\n",
    "data['hashtags'] = data['hashtags'].fillna('').apply(lambda s: s.split(';;;'))\n",
    "data['mentions'] = data['mentions'].fillna('').apply(lambda s: s.split(';;;'))\n",
    "data['urls'] = data['urls'].fillna('').apply(lambda s: s.split(';;;'))\n",
    "# data['text_ht_censored'] = data['text'].apply(lambda t: re.sub(r'http\\S+', '<URL>', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"<HASHTAG>\", t)) \n",
    "data['text'] = data['text'].fillna('').apply(lambda t: re.sub(r'http\\S+', '', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"\", t)).apply(lambda t: re.sub(r'@\\S+', '', t))\n",
    "data['rid'] = data['rid'].combine_first(data['tid'])\n",
    "\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_post.pk'), 'wb') as wf:\n",
    "    pk.dump(data, wf)\n",
    "\n",
    "data_per_user = data.groupby('uid').progress_apply(lambda d: pd.DataFrame({'text' : ' '.join(d['text']),'hashtags': [[e for u in d['hashtags'] for e in u if e != '']],'rid':[list(d['rid'])], 'urls' : [[e for u in d['urls'] for e in u if e != '']] }))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','Moral_Foundation_FrameAxis','inputs',dataset_name+'_per_user.csv'))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','grievancedictionary','inputs',dataset_name+'_per_user.csv'))\n",
    "\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_user.pk'), 'wb') as wf:\n",
    "    pk.dump(data_per_user ,wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d3a5e",
   "metadata": {},
   "source": [
    "### Ausvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e39fc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 273874/273874 [02:32<00:00, 1799.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name='ausvotes'\n",
    "data = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','ausvotes','ausvotes.csv'), dtype=str, header=None, names=['tid','cid','uid','created_at','text','urls', 'urls2'])\n",
    "data['urls'] = data['urls'].fillna('').apply(lambda s: s.split(';;;')) + data['urls2'].fillna('').apply(lambda s: s.split(';;;'))\n",
    "data = data[~data.text.isna()]\n",
    "data['text'] = data['text'].fillna('').apply(lambda t: re.sub(r'http\\S+', '', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"\", t)).apply(lambda t: re.sub(r'@\\S+', '', t))\n",
    "data['rid'] = data['cid'].combine_first(data['tid'])\n",
    "\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_post.pk'), 'wb') as wf:\n",
    "    pk.dump(data, wf)\n",
    "\n",
    "data_per_user = data.groupby('uid').progress_apply(lambda d: pd.DataFrame({'text' : ' '.join(d['text']), 'rid':[list(d['rid'])], 'urls' : [[e for u in d['urls'] for e in u if e != '']] }))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','Moral_Foundation_FrameAxis','inputs',dataset_name+'_per_user.csv'))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','grievancedictionary','inputs',dataset_name+'_per_user.csv'))\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_user.pk'), 'wb') as wf:\n",
    "    pk.dump(data_per_user ,wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585705a6",
   "metadata": {},
   "source": [
    "### Social Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e575856-dd7c-497e-b3ea-e97c167b9ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 49442/49442 [00:21<00:00, 2299.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name='socialsense'\n",
    "\n",
    "fb_data = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','socialsense','fb_for_stance.csv'), dtype=str)[['index','text','text_urls', 'timestamp', 'user']]\n",
    "fb_data['urls'] = fb_data['text_urls'].fillna('').apply(lambda s: s.strip().split(','))\n",
    "fb_data['text'] = fb_data['text'].apply(lambda t: re.sub(r'http\\S+', '', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"\", t)).apply(lambda t: re.sub(r'@\\S+', '', t))\n",
    "fb_data['id'] = fb_data['index']\n",
    "\n",
    "fb_data['uid'] = fb_data['user']\n",
    "fb_data['created_at'] = fb_data['timestamp']\n",
    "fb_data['is_twitter'] = False\n",
    "\n",
    "fb_data = fb_data[['id', 'text','urls', 'uid', 'created_at', 'is_twitter']]\n",
    "\n",
    "id_uid_index = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','socialsense','socialsense_id_uid_index.csv'), dtype=str, header=None, names=['id', 'cid','uid'])[['id','uid']]\n",
    "\n",
    "tw_data = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','socialsense','tw_for_stance_unrolled.csv'), dtype=str)[['id', 'text','text_urls_unrolled', 'date']]\n",
    "tw_data = pd.merge(tw_data,id_uid_index, how='left', on='id')\n",
    "tw_data['urls'] = tw_data['text_urls_unrolled'].fillna('').apply(lambda s: s.strip().split(','))\n",
    "tw_data['text'] = tw_data['text'].apply(lambda t: re.sub(r'http\\S+', '', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"\", t)).apply(lambda t: re.sub(r'@\\S+', '', t))\n",
    "\n",
    "tw_data['created_at'] = tw_data['date']\n",
    "tw_data['is_twitter'] = True\n",
    "\n",
    "tw_data = tw_data[['id', 'text','urls', 'uid', 'created_at', 'is_twitter']]\n",
    "\n",
    "data = pd.concat([tw_data, fb_data])\n",
    "data_filtered = data[~data['uid'].isna()]\n",
    "\n",
    "data_per_user = data_filtered.groupby('uid').progress_apply(lambda d: pd.DataFrame({'text' : ' '.join(d['text']), 'urls' : [[e for u in d['urls'] for e in u if e != '']] }))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','Moral_Foundation_FrameAxis','inputs',dataset_name+'_per_user.csv'))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','grievancedictionary','inputs',dataset_name+'_per_user.csv'))\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_user.pk'), 'wb') as wf:\n",
    "    pk.dump(data_per_user ,wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b80b20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a7b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 574281/574281 [04:20<00:00, 2203.64it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name='riot'\n",
    "data = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','riot','riot.csv'), dtype=str, header=None, names=['tid','cid','uid','created_at','text','urls', 'urls2'])\n",
    "\n",
    "data['urls'] = data['urls'].fillna('').apply(lambda s: s.split(';;;')) + data['urls2'].fillna('').apply(lambda s: s.split(';;;'))\n",
    "data = data[~data.text.isna()]\n",
    "data['text'] = data['text'].apply(lambda t: re.sub(r'http\\S+', '', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"\", t)).apply(lambda t: re.sub(r'@\\S+', '', t))\n",
    "data['rid'] = data['cid'].combine_first(data['tid'])\n",
    "\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_post.pk'), 'wb') as wf:\n",
    "    pk.dump(data, wf)\n",
    "\n",
    "data_per_user = data.groupby('uid').progress_apply(lambda d: pd.DataFrame({'text' : ' '.join(d['text']), 'rid':[list(d['rid'])], 'urls' : [[e for u in d['urls'] for e in u if e != '']] }))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','Moral_Foundation_FrameAxis','inputs',dataset_name+'_per_user.csv'))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','grievancedictionary','inputs',dataset_name+'_per_user.csv'))\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_user.pk'), 'wb') as wf:\n",
    "    pk.dump(data_per_user ,wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5462c",
   "metadata": {},
   "source": [
    "### Parler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66f60df1-ea6a-4417-9bc6-eb2c75c550ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 120048/120048 [00:55<00:00, 2168.63it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name='parler'\n",
    "data = pd.read_csv(os.path.join(base_dir,'data','01_raw_data','parler','parler.csv'), dtype=str, header=None, names=['tid','uid','username','text','created_at','urls'])\n",
    "\n",
    "data['urls'] = data['urls'].fillna('').apply(lambda s: s.split(';'))\n",
    "data = data[~data.text.isna()]\n",
    "data['text'] = data['text'].apply(lambda t: re.sub(r'http\\S+', '', t)).apply(lambda t: re.sub(\"#[A-Za-z0-9_]+\",\"\", t)).apply(lambda t: re.sub(r'@\\S+', '', t))\n",
    "data['rid'] = data['tid']\n",
    "\n",
    "data_per_user = data.groupby('uid').progress_apply(lambda d: pd.DataFrame({'text' : ' '.join(d['text']), 'rid':[list(d['rid'])], 'urls' : [[e for u in d['urls'] for e in u if e != '']] }))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','Moral_Foundation_FrameAxis','inputs',dataset_name+'_per_user.csv'))\n",
    "data_per_user.to_csv(os.path.join(base_dir,'..','grievancedictionary','inputs',dataset_name+'_per_user.csv'))\n",
    "with open(os.path.join(base_dir,'data','01_raw_data',dataset_name,dataset_name+'_per_user.pk'), 'wb') as wf:\n",
    "    pk.dump(data_per_user ,wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce218b88-7806-4208-9982-3b5efc6f94e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rr",
   "language": "python",
   "name": "rr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
